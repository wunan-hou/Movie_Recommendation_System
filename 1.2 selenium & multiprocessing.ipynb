{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Part-2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain all movies' information, write them into a local csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To run this file successfully, you need to download chromdriver:\n",
    "https://chromedriver.chromium.org/\n",
    "* And install selenium package:\n",
    "https://selenium-python.readthedocs.io/installation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from selenium import webdriver      \n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_data_key(review_link):\n",
    "    # get review data key in each page\n",
    "    review_response = requests.get(review_link)\n",
    "    if review_response.status_code == 200:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"HTTP Error\")\n",
    "    review_results_page = BeautifulSoup(review_response.content,'lxml')\n",
    "    data_key = review_results_page.find('div',class_ = 'load-more-data').get('data-key')\n",
    "    new_url = review_link + f'_ajax?paginationKey={data_key}'\n",
    "    return new_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews(review_link,driver):\n",
    "    # this function is the core to use selenium\n",
    "    review_list = []\n",
    "    review_response = requests.get(review_link)\n",
    "    if review_response.status_code == 200:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"HTTP Error\")\n",
    "    review_results_page = BeautifulSoup(review_response.content,'lxml')\n",
    "    for review in review_results_page.find_all('div',class_ = 'text show-more__control'):\n",
    "        review_list.append(review.get_text())\n",
    "    if review_results_page.find('button', class_ = 'ipl-load-more__button'):\n",
    "        driver.get(review_link)\n",
    "        while len(review_list)<30:\n",
    "            loadMoreButton = driver.find_element_by_class_name('ipl-load-more__button')\n",
    "            # get review for this page\n",
    "            review_response = requests.get(review_data_key(review_link))\n",
    "            if review_response.status_code == 200:\n",
    "                pass\n",
    "            else:\n",
    "                print(\"HTTP Error\")\n",
    "            review_results_page = BeautifulSoup(review_response.content,'lxml')\n",
    "            for review in review_results_page.find_all('div',class_ = 'text show-more__control'):\n",
    "                review_list.append(review.get_text())\n",
    "            loadMoreButton.click()\n",
    "        review_list = review_list[:31]\n",
    "    return review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_film_feature(movie_link_list,driver):\n",
    "    # this function will give back all information about every movie\n",
    "    dict_ = {}\n",
    "    for movie in movie_link_list:\n",
    "        movie_name = movie[0]\n",
    "        movie_link = movie[1]\n",
    "        dict_[movie_name] = {}\n",
    "        response = requests.get(movie_link)\n",
    "        if response.status_code == 200:\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Failure\")\n",
    "        results_page = BeautifulSoup(response.content,'lxml')\n",
    "        \n",
    "        try:\n",
    "            # get rating_value\n",
    "            dict_[movie_name]['rating_value'] = results_page.find('div',class_ = 'ratingValue').find('span').get_text()\n",
    "            # get time\n",
    "            dict_[movie_name]['time'] = results_page.find('div',class_ = 'subtext').find('time').get_text().strip()\n",
    "            # get genres\n",
    "            genres = []\n",
    "            for i in results_page.find('div',class_ = 'subtext').find_all('a')[:-1]:\n",
    "                genres.append(i.get_text().strip())\n",
    "            dict_[movie_name]['genres'] = genres\n",
    "            # get release_date\n",
    "            dict_[movie_name]['release_date'] = ' '.join(results_page.find('div',class_ = 'subtext').find_all('a')[-1].get_text().strip().split()[:-1])\n",
    "            # get release_country\n",
    "            dict_[movie_name]['release_country'] = results_page.find('div',{'class':'article','id':'titleDetails'}).find_all('div',{'class':'txt-block'})[1].get_text().strip().split('\\n')\n",
    "            if '|' in dict_[movie_name]['release_country']:\n",
    "                dict_[movie_name]['release_country'].remove('|')\n",
    "            dict_[movie_name]['release_country'] = dict_[movie_name]['release_country'][1:]  \n",
    "            # preparation for getting director,writers,stars\n",
    "            pattern = re.compile(r'\\d{1} more credits')\n",
    "            # get director\n",
    "            director_writers_stars_0 = results_page.find_all('div',class_ = 'credit_summary_item')[0].get_text()\n",
    "            if pattern.findall(director_writers_stars_0):\n",
    "                director_link = movie_link + results_page.find_all('div',class_ = 'credit_summary_item')[0].find_all('a')[-1].get('href')\n",
    "                director_response = requests.get(director_link)\n",
    "                if not director_response.status_code == 200:\n",
    "                     print(\"HTTP Error\")\n",
    "                director_results_page = BeautifulSoup(director_response.content,'lxml')\n",
    "                dict_[movie_name]['director'] = [director_name.get_text().strip() for director_name in director_results_page.find_all('table',class_='simpleTable simpleCreditsTable')[0].find_all('td',class_='name')]\n",
    "            else:\n",
    "                dict_[movie_name]['director'] = results_page.find_all('div',class_ = 'credit_summary_item')[0].get_text().split('\\n')[2].strip('|').strip(' ').split(',')\n",
    "            # get writers\n",
    "            director_writers_stars_1 = results_page.find_all('div',class_ = 'credit_summary_item')[1].get_text()\n",
    "            if pattern.findall(director_writers_stars_1):\n",
    "                writers_link = movie_link + results_page.find_all('div',class_ = 'credit_summary_item')[1].find_all('a')[-1].get('href')\n",
    "                writers_response = requests.get(director_link)\n",
    "                if not writers_response.status_code == 200:\n",
    "                     print(\"HTTP Error\")\n",
    "                writers_results_page = BeautifulSoup(writers_response.content,'lxml')\n",
    "                dict_[movie_name]['writers'] = [writers_name.get_text().strip() for writers_name in writers_results_page.find_all('table',class_='simpleTable simpleCreditsTable')[1].find_all('td',class_='name')]\n",
    "            else:\n",
    "                dict_[movie_name]['writers'] = results_page.find_all('div',class_ = 'credit_summary_item')[1].get_text().split('\\n')[2].strip('|').strip(' ').split(',')\n",
    "            # get stars\n",
    "            stars_link = movie_link + results_page.find_all('div',class_ = 'credit_summary_item')[2].find_all('a')[-1].get('href')\n",
    "            stars_response = requests.get(stars_link)\n",
    "            if not stars_response.status_code == 200:\n",
    "                print(\"HTTP Error\")\n",
    "            stars_results_page = BeautifulSoup(stars_response.content,'lxml')\n",
    "            dict_[movie_name]['stars'] = [stars.find_all('td')[1].get_text().strip() for stars in stars_results_page.find('table',class_='cast_list').find_all('tr') if not len(stars.find_all('td')) == 1]\n",
    "            # get storyline\n",
    "            dict_[movie_name]['storyline'] = results_page.find('div',class_ = 'inline canwrap').find('span').get_text().strip()\n",
    "            # get keywords\n",
    "            url = 'https://www.imdb.com'\n",
    "            try:\n",
    "                keywords_url = url + results_page.find('div',class_ = 'see-more inline canwrap').find('nobr').find('a').get('href')\n",
    "                keywords_response = requests.get(keywords_url)\n",
    "                if keywords_response.status_code == 200:\n",
    "                    pass\n",
    "                else:\n",
    "                    print(\"HTTP Error\")\n",
    "                keywords_results_page = BeautifulSoup(keywords_response.content,'lxml') \n",
    "                dict_[movie_name]['keywords'] = [ i.get_text().strip() for i in keywords_results_page.find_all('div',class_ = 'sodatext')]\n",
    "            except:\n",
    "                dict_[movie_name]['keywords'] = None\n",
    "            # get reviews\n",
    "            # in this part, we use selenium to help us cilck on button automatically\n",
    "            review_link = movie_link + 'reviews/'\n",
    "            dict_[movie_name]['reviews'] = get_reviews(review_link,driver)\n",
    "        except:\n",
    "            pass\n",
    "    driver.quit()\n",
    "    return dict_    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_func(movie_link_list):\n",
    "    # write a big function to run all codes before\n",
    "    # thus we can use this function in multiprocessing\n",
    "    driver = webdriver.Chrome('/Users/fancy/Downloads/chromedriver') # change the path to where the chromedriver is located\n",
    "    result = imdb_film_feature(movie_link_list,driver)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from csv and prepare it to use in multiprocessing\n",
    "test = pd.read_csv('movie_unique')\n",
    "movie_link_list_1 = test[['name','link']].values.tolist()[90000:105000]  # change the index here to obtain data you want to run\n",
    "movie_link_list_2 = test[['name','link']].values.tolist()[105000:120000]\n",
    "movie_link_list_3 = test[['name','link']].values.tolist()[120000:135000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code is the core of multuprocessing\n",
    "# change the int value of Pool() to limit the maxmium processing you can run\n",
    "with Pool(5) as p:\n",
    "    t = p.map(big_func, [movie_link_list_1,movie_link_list_2,movie_link_list_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code is to obtain result from multiprocessing\n",
    "# and write the result into a csv file\n",
    "d1= pd.DataFrame(t[0]).T\n",
    "d1.to_csv('90000-105000')\n",
    "d2 = pd.DataFrame(t[1]).T\n",
    "d2.to_csv('105000-120000')\n",
    "d3 = pd.DataFrame(t[2]).T\n",
    "d3.to_csv('120000-135000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hint: \n",
    "* because the data we obtain from these functions is seperate, so we use below code to obtain a complete database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use variable d1 to d41 to read all seperate file we have obtained\n",
    "# for example:\n",
    "d1 = pd.read_csv('0-20',index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat them into a dataframe\n",
    "data = pd.concat([d1,d2,d3,d4,d5,d7,d6,d8,d9,\n",
    "                 d10,d11,d12,d13,d14,d15,d16,d17,d18,d19,\n",
    "                 d20,d21,d22,d23,d24,d25,d26,d27,d28,d29,\n",
    "                 d30,d31,d32,d33,d34,d35,d36,d37,d38,d39,\n",
    "                 d40,d41])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we write it into a csv, we do some basic cleaning\n",
    "# drop rows with all None value\n",
    "data = data.dropna(how='all')\n",
    "# drop dupliates \n",
    "data = data.drop_duplicates(subset='index',keep='first')\n",
    "# set column 'index' as index of datafram\n",
    "data.set_index('index')\n",
    "# write the data to csv file\n",
    "data.to_csv('data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
